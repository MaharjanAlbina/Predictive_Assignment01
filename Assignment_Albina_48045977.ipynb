{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BUSA8001- Programming Task 1  \n",
    "\n",
    "**Assignment Points**: 100  \n",
    "**Submission**: Provide your answers in this notebook and submit it via iLearn\n",
    "\n",
    "- Where a question requires a written answer provide your solution in Markdown in the cells under each question.\n",
    "- Comment out your print statements unless you are explicitly asked to print your output. \n",
    "- 5 marks will be deducted for printed outputs that are not asked for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Assignment\n",
    "\n",
    "- For this assignment there are two files in the `data` folder `credit_record.csv` and `application_record.csv` where bank clients are related by the `ID` column.\n",
    "\n",
    "- In `application_record.csv` we have the following variables\n",
    "\n",
    "| Feature Name         | Explanation     | Additional Remarks |\n",
    "|--------------|-----------|-----------|\n",
    "| ID | Randomly allocated client number      |         |\n",
    "| AMT_INCOME   | Annual income  |  |\n",
    "| NAME_INCOME_TYPE   | Income Source |  |\n",
    "| NAME_EDUCATION_TYPE   | Level of Education  |  |\n",
    "| CODE_GENDER   | Applicant's Gender   |  |\n",
    "| FLAG_OWN_CAR | Car Ownership |  | \n",
    "| CNT_CHILDREN | Number of Children | |\n",
    "| FLAG_OWN_REALTY | Real Estate Ownership | | \n",
    "| NAME_FAMILY_STATUS | Relationship Status | | \n",
    "| NAME_HOUSING_TYPE | Housing Type | | \n",
    "| DAYS_BIRTH | No. of Days | Count backwards from current day (0), -1 means yesterday\n",
    "| DAYS_EMPLOYED | No. of Days | Count backwards from current day (0). If positive, it means the person is currently unemployed.\n",
    "| FLAG_MOBIL | Mobile Phone Ownership | | \n",
    "| FLAG_WORK_PHONE | Work Phone Ownership | | \n",
    "| FLAG_PHONE | Landline Phone Ownership | | \n",
    "| FLAG_EMAIL | Landline Phone Ownership | | \n",
    "| OCCUPATION_TYPE | Occupation | | \n",
    "| CNT_FAM_MEMBERS | Count of Family Members | |\n",
    "\n",
    "\n",
    "\n",
    "- In `credit_record.csv` we have the following variables\n",
    "\n",
    "\n",
    "| Feature Name         | Explanation     | Additional Remarks |\n",
    "|--------------|-----------|-----------|\n",
    "| ID | Randomly allocated client number | |\n",
    "| MONTHS_BALANCE | Number of months in the past from now when STATUS is measured | 0 = current month, -1 = last month, -2 = two months ago, etc.|\n",
    "| STATUS | Number of days a payment is past due | 0: 1-29 days past due 1: 30-59 days past due 2: 60-89 days overdue 3: 90-119 days overdue 4: 120-149 days overdue 5: Overdue or bad debts, write-offs for more than 150 days C: paid off that month X: No loan for the month |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Task 1: Reading, Summarising and Cleaning Data (Total Marks: 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** \n",
    "\n",
    "1. Import the `application_record.csv` and `credit_record.csv` files from `data` folder into pandas DataFrames named `df_application` and `df_credit`, respectively. (1 mark)\n",
    "\n",
    "2. How many rows are there in `df_application` and `df_credit`, respectively? Provide your answers with print() and state them in Markdown text. (1 mark)\n",
    "\n",
    "3. How many unique bank clients are there in `df_application` and `df_credit`? Provide your answers with print() and state them in Markdown text. (1 mark)\n",
    "\n",
    "4. Add the records from `df_credit` to `df_application` by merging the data from the two DataFrames on the `ID` column, and output the joint data into a new DataFrame named `df`. Hint: Use `merge` function from pandas by setting `how` parameter to `inner` (4 marks) \n",
    "\n",
    "5. How many rows and how many unique clients are there now in `df`? (1 mark)\n",
    "\n",
    "6. How are multiple rows for each `ID` in `df` different? Answer in Markdown text. (2 mark) \n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 1. Load the CSV files into pandas DataFrames\n",
    "df_application = pd.read_csv('/Users/albina/Documents/Macquarie Uni/Sem 3/Applied Predictive Analytics/Assignment1-S2-2024/data/application_record.csv')\n",
    "df_credit = pd.read_csv('/Users/albina/Documents/Macquarie Uni/Sem 3/Applied Predictive Analytics/Assignment1-S2-2024/data/credit_record.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_application: 438445\n",
      "Number of rows in df_credit: 1047185\n"
     ]
    }
   ],
   "source": [
    "# 2. Number of rows in df_application\n",
    "rows_application = len(df_application)\n",
    "print(\"Number of rows in df_application:\", rows_application)\n",
    "\n",
    "# Number of rows in df_credit\n",
    "rows_credit = len(df_credit)\n",
    "print(\"Number of rows in df_credit:\", rows_credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ans\n",
    "There are 438445 number of rows in df_application.\n",
    "There are 1047185 number of rows in df_credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique clients in df_application: 438398\n",
      "Number of unique clients in df_credit: 45924\n"
     ]
    }
   ],
   "source": [
    "#3. Number of unique clients in df_application\n",
    "unique_clients_application = df_application['ID'].nunique()\n",
    "print(\"Number of unique clients in df_application:\", unique_clients_application)\n",
    "\n",
    "# Number of unique clients in df_credit\n",
    "unique_clients_credit = df_credit['ID'].nunique()\n",
    "print(\"Number of unique clients in df_credit:\", unique_clients_credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ans\n",
    "There are 438398 number of unique bank clients in df_application.\n",
    "\n",
    "There are 45924 number of unique bank clients in df_credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. \n",
    "\n",
    "# Merging the two DataFrames\n",
    "df = pd.merge(df_application, df_credit, on='ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df: 776325\n",
      "Number of unique clients in df: 36396\n"
     ]
    }
   ],
   "source": [
    "# 5. Number of rows in df\n",
    "num_rows = len(df)\n",
    "print(\"Number of rows in df:\", num_rows)\n",
    "\n",
    "# Number of unique clients in the merged DataFrame df\n",
    "unique_clients = df['ID'].nunique()\n",
    "print(\"Number of unique clients in df:\", unique_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ans\n",
    "There are 776325 number of rows in df.\n",
    "\n",
    "There are 36396 number of uniquie clients in df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How are the multiple rows for each ID in  df different ?\n",
    "\n",
    "# Ans : \n",
    "The Multiple rows for each ID in df different because of following reasons:\n",
    "\n",
    "While merging 'df_application' and 'df_credit', each rows in 'df' represents a combination of client information from 'df_application' and monthly credit status from 'df_credit'. \n",
    "\n",
    "The pd.merge() function merges two Dataframes based on specified column here i.e. ID which is common in both 'df_application' and df_credit'.\n",
    "\n",
    "For every ID that exists in both DataFrames, the corresponding rows from both df_application and df_credit are combined into a single row in the resulting DataFrame df with the use of parrameter to inner.\n",
    "\n",
    "Rows Excluded: Any 'ID' that is present in only one of the Dataframes (either 'df_application' or 'df_credit') is excluded from the result. This ensures that df contains only complete information about clients who have both demographic data and credit history.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 2.**\n",
    "\n",
    "1. Change the values of `STATUS` in `df` according to the following mapping: {C, X, 0} -> 0 and {1, 2, 3, 4, 5} -> 1 making sure that the new values of 0 and 1 are encoded as integers. (2 marks)\n",
    "\n",
    "2. Create a new *numpy* array called `list_of_past_due` that includes the unique ID numbers of clients whose `STATUS = 1` at any point during the last 12 months (hint: count the current month as the first month). (2 marks) \n",
    "\n",
    "3. Create a new DataFrame called `df_final` that contains the rows of `df` for which the `ID` is in `list_of_past_due`, keeping only one row for each `ID` (hint: keep the first duplicate row). How many rows do you have in `df_final`? Answer using both print() function and in Markdown text. (hint: find out about `isin()` function in pandas.) (2 marks)\n",
    "\n",
    "4. Add a new column `y = 1` for all the rows in `df_final`. (1 marks)\n",
    "\n",
    "5. Increase `df_final` to a total of 4,500 rows by adding rows from `df` with unique `ID`s which are not in `list_of_past_due`. To do this start adding the rows from the beginning of `df`. (hint: learn what `~`, i.e. tilde sign, does in pandas). (2 marks) \n",
    "\n",
    "6. Fill the missing values of `y` in `df_final` with zeros. Remove `STATUS` and `MONTHS_BALANCE` from `df_final`. (1 mark)\n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 \n",
    "# Define the mapping for STATUS values\n",
    "status_mapping = {'C': 0, 'X': 0, '0': 0, '1': 1, '2': 1, '3': 1, '4': 1, '5': 1}\n",
    "\n",
    "# Apply the mapping to the STATUS column in df\n",
    "df['STATUS'] = df['STATUS'].map(status_mapping)\n",
    "\n",
    "# Convert the STATUS column to integers to ensure they are encoded as integers\n",
    "df['STATUS'] = df['STATUS'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "import numpy as np\n",
    "\n",
    "# Filter rows where STATUS is 1 and MONTHS_BALANCE is within the last 12 months (0 to -11)\n",
    "past_due_clients = df[(df['STATUS'] == 1) & (df['MONTHS_BALANCE'] >= -11)]\n",
    "\n",
    "# Extract unique IDs from the filtered rows\n",
    "list_of_past_due = np.unique(past_due_clients['ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_final: 1737\n",
      "           ID CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
      "369   5008831           F            N               Y           NaN   \n",
      "954   5008872           M            Y               Y           NaN   \n",
      "1718  5008912           F            N               Y           0.0   \n",
      "1755  5008914           F            N               Y           0.0   \n",
      "1790  5008916           F            N               Y           0.0   \n",
      "\n",
      "      AMT_INCOME      NAME_INCOME_TYPE            NAME_EDUCATION_TYPE  \\\n",
      "369     157500.0               Working  Secondary / secondary special   \n",
      "954     360000.0  Commercial associate                            NaN   \n",
      "1718    297000.0  Commercial associate                            NaN   \n",
      "1755    297000.0  Commercial associate                            NaN   \n",
      "1790    297000.0  Commercial associate                            NaN   \n",
      "\n",
      "        NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
      "369                Married  House / apartment      -10031          -1469   \n",
      "954                Married  House / apartment      -16670          -5364   \n",
      "1718  Single / not married   Rented apartment      -15519          -3234   \n",
      "1755  Single / not married   Rented apartment      -15519          -3234   \n",
      "1790  Single / not married   Rented apartment      -15519          -3234   \n",
      "\n",
      "      FLAG_MOBIL  FLAG_WORK_PHONE  FLAG_PHONE  FLAG_EMAIL OCCUPATION_TYPE  \\\n",
      "369            1                0           1           0        Laborers   \n",
      "954            1                0           1           0  Security staff   \n",
      "1718           1                0           0           0        Laborers   \n",
      "1755           1                0           0           0        Laborers   \n",
      "1790           1                0           0           0        Laborers   \n",
      "\n",
      "      CNT_FAM_MEMBERS  MONTHS_BALANCE  STATUS  \n",
      "369                 2               0       0  \n",
      "954                 2               0       0  \n",
      "1718                1               0       0  \n",
      "1755                1               0       0  \n",
      "1790                1               0       0  \n"
     ]
    }
   ],
   "source": [
    "#3.  Create df_final by filtering rows in df where the ID is in list_of_past_due\n",
    "df_final = df[df['ID'].isin(list_of_past_due)]\n",
    "\n",
    "# Keep only the first occurrence of each ID\n",
    "df_final = df_final.drop_duplicates(subset=['ID'], keep='first')\n",
    "\n",
    "# Display the number of rows in df_final\n",
    "print(\"Number of rows in df_final:\", len(df_final))\n",
    "\n",
    "# Display the first few rows of df_final to confirm the result\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ans\n",
    "\n",
    "The code filters rows from a DataFrame df where the ID matches entries in list_of_past_due, then removes duplicate IDs, keeping only the first occurrence. The resulting DataFrame, df_final, contains 1,737 rows with unique IDs. The code also outputs the number of rows and displays a preview of the filtered data, which includes client demographic and financial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "# Add a new column y with value 1 for all rows\n",
    "df_final['y'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Find IDs that are not in list_of_past_due\n",
    "non_past_due = df[~df['ID'].isin(list_of_past_due)]\n",
    "\n",
    "# Keep adding rows until df_final has 4,500 rows\n",
    "rows_to_add = 4500 - len(df_final)\n",
    "additional_rows = non_past_due.drop_duplicates(subset=['ID']).iloc[:rows_to_add]\n",
    "\n",
    "# Append these rows to df_final\n",
    "df_final = pd.concat([df_final, additional_rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.\n",
    "#  Fill missing values in y with 0\n",
    "df_final['y'] = df_final['y'].fillna(0)\n",
    "\n",
    "# Remove STATUS and MONTHS_BALANCE columns\n",
    "df_final = df_final.drop(['STATUS', 'MONTHS_BALANCE'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Question 3**. \n",
    "1. Delete `ID` column from `df_final` and reset its index. (1 marks)\n",
    "2. Assuming that `NAME_EDUCATION_TYPE` is the only ordinal variable in `df_final`, which variables are numeric and which ones are nominal? Answer this question by copying and completing the following table (6 marks)\n",
    "\n",
    "|Variable type|Numbers of features|Features' list|\n",
    "| --- | --- | --- |\n",
    "|Numeric:|||\n",
    "|Ordinal:|1| NAME_EDUCATION_TYPE |\n",
    "|Nominal:|||\n",
    "\n",
    "3. Using appropriate functions find and comment on the missing values in `df_final` (3 marks)   \n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Delete the ID column from df_final and reset its index\n",
    "df_final = df_final.drop(columns=['ID']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 3 .2 \n",
    "\n",
    "\n",
    "\n",
    "| Variable Type | Number of Features | Features' List                                |\n",
    "|---------------|--------------------|-----------------------------------------------|\n",
    "| Numeric       | 5                  | CNT_CHILDREN, AMT_INCOME, DAYS_BIRTH, DAYS_EMPLOYED, CNT_FAM_MEMBERS|\n",
    "| Ordinal       | 1                  | NAME_EDUCATION_TYPE                          |\n",
    "| Nominal       | 12                 | CODE_GENDER, FLAG_OWN_CAR, FLAG_OWN_REALITY, NAME_INCOME_TYPE, NAME_FAMILY_STATUS, NAME_HOUSING_TYPE, FLAG_MOBIL, FLAG_WORK_PHONE, FLAG_PHONE, FLAG_EMAIL, OCCUPATION_TYPE, y |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in df_final:\n",
      " CODE_GENDER               0\n",
      "FLAG_OWN_CAR              0\n",
      "FLAG_OWN_REALTY           0\n",
      "CNT_CHILDREN             74\n",
      "AMT_INCOME                0\n",
      "NAME_INCOME_TYPE          0\n",
      "NAME_EDUCATION_TYPE    1831\n",
      "NAME_FAMILY_STATUS        0\n",
      "NAME_HOUSING_TYPE         0\n",
      "DAYS_BIRTH                0\n",
      "DAYS_EMPLOYED             0\n",
      "FLAG_MOBIL                0\n",
      "FLAG_WORK_PHONE           0\n",
      "FLAG_PHONE                0\n",
      "FLAG_EMAIL                0\n",
      "OCCUPATION_TYPE        1354\n",
      "CNT_FAM_MEMBERS           0\n",
      "y                         0\n",
      "dtype: int64\n",
      "\n",
      "Comment:\n",
      "The output shows the count of missing values in each column of df_final. Columns with significant missing data may need imputation or removal, depending on their relevance.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "# Check for missing values in df_final\n",
    "missing_values = df_final.isnull().sum()\n",
    "\n",
    "# Display the missing values\n",
    "print(\"Missing values in df_final:\\n\", missing_values)\n",
    "\n",
    "# Comment on the missing values\n",
    "print(\"\\nComment:\")\n",
    "print(\"The output shows the count of missing values in each column of df_final. Columns with significant missing data may need imputation or removal, depending on their relevance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Task 2: Imputing missing values and dealing with categorical features (Total Marks: 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** \n",
    "1. Use an appropriate `pandas` function to impute missing values in `df_final` (15 marks)\n",
    "    - Take into consideration the type of each variable and the best practices we discussed in class/lecture notes\n",
    "\n",
    "(Total: 15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to impute missing values based on variable type\n",
    "def impute_missing_values(df):\n",
    "    for column in df.columns:\n",
    "        # If the column is numeric, fill missing values with the mean\n",
    "        if df[column].dtype in ['int64', 'float64']:\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "        # If the column is categorical (nominal/ordinal), fill missing values with the mode\n",
    "        else:\n",
    "            df[column] = df[column].fillna(df[column].mode()[0])\n",
    "    return df\n",
    "\n",
    "# Impute missing values in df_final\n",
    "df_final = impute_missing_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " ## Mean/Median/Mode Interpolation    \n",
    " \n",
    " In our data set we have encountered situtations where *categorical* and *numerical* data is missing   \n",
    " - Mean/Median/Mode Interpolation: Replace missing values with the mean, median, or mode of the column\n",
    " - However we have to be careful about the type of data we have\n",
    "     - **Numeric data** -> I have used mean (if concerned about outliers or when the distribution is not symmetrical)    \n",
    "     - **Categorical data** -> I have used mode (the only value that can be used with nominal data that can't be ordered)  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Question 5**. Convert the values in `NAME_EDUCATION_TYPE` as follows\n",
    "- Lower secondary -> 1\n",
    "- Secondary / secondary special -> 2\n",
    "- Incomplete higher -> 3\n",
    "- Higher education -> 4\n",
    "\n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "# Define the mapping for NAME_EDUCATION_TYPE values\n",
    "education_mapping = {\n",
    "    'Lower secondary': 1,\n",
    "    'Secondary / secondary special': 2,\n",
    "    'Incomplete higher': 3,\n",
    "    'Higher education': 4\n",
    "}\n",
    "\n",
    "# Apply the mapping to the NAME_EDUCATION_TYPE column\n",
    "df_final['NAME_EDUCATION_TYPE'] = df_final['NAME_EDUCATION_TYPE'].replace(education_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Question 6**. \n",
    "\n",
    "Add dummy variables to `df_final` for all of the nominal features which are currently stored as string (text). \n",
    "- Make sure to delete the original variables from the dataframe\n",
    "- Drop the first column from each set of created dummy variable, i.e. for each feature\n",
    "\n",
    "\n",
    "\n",
    "(Total: 10 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CNT_CHILDREN  AMT_INCOME  NAME_EDUCATION_TYPE  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
      "0      0.403525    157500.0                    2      -10031          -1469   \n",
      "1      0.403525    360000.0                    2      -16670          -5364   \n",
      "2      0.000000    297000.0                    2      -15519          -3234   \n",
      "3      0.000000    297000.0                    2      -15519          -3234   \n",
      "4      0.000000    297000.0                    2      -15519          -3234   \n",
      "\n",
      "   FLAG_MOBIL  FLAG_WORK_PHONE  FLAG_PHONE  FLAG_EMAIL  CNT_FAM_MEMBERS  ...  \\\n",
      "0           1                0           1           0                2  ...   \n",
      "1           1                0           1           0                2  ...   \n",
      "2           1                0           0           0                1  ...   \n",
      "3           1                0           0           0                1  ...   \n",
      "4           1                0           0           0                1  ...   \n",
      "\n",
      "   OCCUPATION_TYPE_Laborers  OCCUPATION_TYPE_Low-skill Laborers  \\\n",
      "0                      True                               False   \n",
      "1                     False                               False   \n",
      "2                      True                               False   \n",
      "3                      True                               False   \n",
      "4                      True                               False   \n",
      "\n",
      "   OCCUPATION_TYPE_Managers  OCCUPATION_TYPE_Medicine staff  \\\n",
      "0                     False                           False   \n",
      "1                     False                           False   \n",
      "2                     False                           False   \n",
      "3                     False                           False   \n",
      "4                     False                           False   \n",
      "\n",
      "   OCCUPATION_TYPE_Private service staff  OCCUPATION_TYPE_Realty agents  \\\n",
      "0                                  False                          False   \n",
      "1                                  False                          False   \n",
      "2                                  False                          False   \n",
      "3                                  False                          False   \n",
      "4                                  False                          False   \n",
      "\n",
      "   OCCUPATION_TYPE_Sales staff  OCCUPATION_TYPE_Secretaries  \\\n",
      "0                        False                        False   \n",
      "1                        False                        False   \n",
      "2                        False                        False   \n",
      "3                        False                        False   \n",
      "4                        False                        False   \n",
      "\n",
      "   OCCUPATION_TYPE_Security staff  OCCUPATION_TYPE_Waiters/barmen staff  \n",
      "0                           False                                 False  \n",
      "1                            True                                 False  \n",
      "2                           False                                 False  \n",
      "3                           False                                 False  \n",
      "4                           False                                 False  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Select nominal columns based on the given table\n",
    "nominal_columns = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', \n",
    "                   'NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', \n",
    "                   'OCCUPATION_TYPE']\n",
    "\n",
    "# Create dummy variables and drop the first column of each set to avoid multicollinearity\n",
    "df_final = pd.get_dummies(df_final, columns=nominal_columns, drop_first=True)\n",
    "\n",
    "# Display the first few rows of the modified DataFrame to confirm the changes\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Task 3 Preparing X and y arrays (Total Marks: 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**. \n",
    "\n",
    "1. Create a numpy array named `y` from the `y` column of `df_final` making sure that the values of the array `y` are stored as integers (3 marks)   \n",
    "2. Create a numpy array named `X`  from all the remaining features in `df_final` (2 marks)   \n",
    "\n",
    "(Total: 5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Create a numpy array named y from the 'y' column, ensuring values are stored as integers\n",
    "y = df_final['y'].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Create a numpy array named X from all the remaining features in df_final\n",
    "X = df_final.drop(columns=['y']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Question 8**. \n",
    "\n",
    "1. Use an appropriate scikit-learn library we used in class to create `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 75% train and 25% test datasets (2.5 marks) \n",
    "    - Set random_state to 8 and stratify the subsamples so that train and test datasets have roughly equal proportions of the target's class labels \n",
    "2. Standardise the data using `StandardScaler` library (2.5 marks)   \n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training (75%) and test (25%) sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=8, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initializing the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting the scaler only on the training data and transforming it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transforming the test data using the already fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Task 4. Logistic Regression and Random Forest Classifiers and Accuracies (Total Marks: 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**. \n",
    "\n",
    "1. Train a Logistic Regression Classifier on standardised data (5 marks)\n",
    "    - Set `random_state` to 10 (don't change any other parameters)\n",
    "    - Compute and print training and test dataset accuracies   \n",
    "2. Train a Random Forest Classifier on standardised data (5 marks)\n",
    "    - Set `random_state` to 10 (don't change any other parameters)\n",
    "    - Compute and print training and test dataset accuracies\n",
    "\n",
    "When printing accuracies round the values to three decimal places.      \n",
    "\n",
    "(Total: 10 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy: 0.657\n",
      "Logistic Regression Test Accuracy: 0.658\n"
     ]
    }
   ],
   "source": [
    "# 1. \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize Logistic Regression with random_state set to 10\n",
    "log_reg = LogisticRegression(random_state=10)\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_log = log_reg.predict(X_train_scaled)\n",
    "y_test_pred_log = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print accuracies\n",
    "train_accuracy_log = accuracy_score(y_train, y_train_pred_log)\n",
    "test_accuracy_log = accuracy_score(y_test, y_test_pred_log)\n",
    "\n",
    "print(f\"Logistic Regression Train Accuracy: {round(train_accuracy_log, 3)}\")\n",
    "print(f\"Logistic Regression Test Accuracy: {round(test_accuracy_log, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Accuracy: 0.977\n",
      "Random Forest Test Accuracy: 0.898\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest with random_state set to 10\n",
    "rf_clf = RandomForestClassifier(random_state=10)\n",
    "\n",
    "# Train the model\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_clf.predict(X_train_scaled)\n",
    "y_test_pred_rf = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print accuracies\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Train Accuracy: {round(train_accuracy_rf, 3)}\")\n",
    "print(f\"Random Forest Test Accuracy: {round(test_accuracy_rf, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**. \n",
    "\n",
    "a) Comment and compare the training and test accuracies for each classifier computed in Question 9. What can we say about the extent of overfitting for each classifier? (5 marks)   \n",
    "b) Comment and compare the accuracies across the two classifiers. Which classifier provides better forecasts? (5 marks)   \n",
    "c) What can you say about the presence of nonlinearities in the dataset? (10 marks)   \n",
    "\n",
    "(Total: 20 marks)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Comment and compare the training and test accuracies for each classifier. What can we say about the extent of overfitting for each classifier?\n",
    "1. Logistic Regression:\n",
    "Train Accuracy: 0.657\n",
    "Test Accuracy: 0.658\n",
    "The train and test accuracies are very close, indicating that the model is not overfitting. This suggests that Logistic Regression has a good generalization capability on the data.\n",
    "Logistic Regression typically has similar training and test accuracies if the model is well-fitted and not overfitting, indicating that Logistic Regression is managing the complexity of the data well and not learning noise.\n",
    "\n",
    "2. Random Forest:\n",
    "Train Accuracy: 0.977\n",
    "Test Accuracy: 0.898\n",
    "The train accuracy is much higher than the test accuracy, indicating significant overfitting. The model fits the training data very well but struggles to generalize on the test data.\n",
    "Random Forest often achieves very high training accuracy because it uses multiple decision trees, each capturing different patterns.\n",
    "The test accuracy is significantly lower than the training accuracy, this indicates overfitting.\n",
    "Overfitting: If the gap between training and test accuracy is large, it suggests that Random Forest is overfitting by capturing complex patterns in the training data that don't generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Comment and compare the accuracies across the two classifiers. Which classifier provides better forecasts?\n",
    "Comparison:\n",
    "Logistic Regression shows consistent but moderate performance on both training and test data.\n",
    "Random Forest performs exceptionally well on training data but less so on test data due to overfitting.\n",
    "\n",
    "Better Forecasts:\n",
    "Despite overfitting, Random Forest has a higher test accuracy (0.898) compared to Logistic Regression (0.658), indicating that it provides better forecasts on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) What can you say about the presence of non-linearities in the dataset?\n",
    "\n",
    "Nonlinearities:\n",
    "Nonlinear relationships, occur when the effect of one variable on another is not constant, meaning the change is not proportional or predictable by a straight line. Nonlinear patterns can include curves, exponential growth, quadratic relationships, and other complex forms that cannot be captured by a simple straight line.\n",
    "If Random Forest significantly outperforms Logistic Regression, especially on the test data, it indicates the presence of nonlinear relationships within the data. Random Forest’s ability to capture complex interactions and non-linear patterns often gives it an edge in such cases.\n",
    "\n",
    "In the context of dataset, the Random Forest’s better performance compared to Logistic Regression suggests that there are nonlinearities present—patterns or relationships in the data that are not adequately captured by a linear model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
